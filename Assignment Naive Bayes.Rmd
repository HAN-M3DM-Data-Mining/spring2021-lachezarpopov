---
title: "ssigment - Naive Bayes DIY"
author:
  - Lachezar Popov - Author
  - name reviewer here - Reviewer
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
  html_notebook:
    toc: yes
    toc_depth: 2
---

```{r results = "hide"}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```

---

Choose a suitable dataset from [this](https://github.com/HAN-M3DM-Data-Mining/assignments/tree/master/datasets) folder and train your own Naive Bayes model. Follow all the steps from the CRISP-DM model.

**Chosen dataset**: Fake News

## Business Understanding
text and code here

## Data Understanding

The dataset was retrived from Kaggle, which is a community-maintained repository for public dataset. The data is comprised of news articles (the instances) along with labels for those articles (trustworthy or not) and a few other variables.

First we will look at the first 10 rows of the dataset to get a better understanding of the data.

```{r}
#fakeNews_url = 'https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv'
#rawDF = read_csv(fakeNews_url)
head(rawDF, 10) 
```

We can see that the dataset has five columns. The labels are market with 1 (unreliable) and 0 (reliable). The other variables are the ID of the article (numeric), title (character), author (character) and the content of the aricle in the text column (character).

In the summary of the dataframe we can see that there are 20800 observations.

```{r}
summary(rawDF)
```

Let us also examine the most commonly occuring words in reliable and unreliable articles through a wordcloud.

```{r}
unreliable = rawDF %>% filter(label == 1)
reliable = rawDF %>% filter(label == 0)

wordcloud(unreliable$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(reliable$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))
```

The output produces some interesting results. For example, we can see that the word "trump" appears in both wordclouds. Most of the words, however, are generic and do not provide much information (e.g. "the", "one", "can", "just", etc.).

## Data Preparation

Fist we will transform the data type of the target from character to factor.

```{r}
rawDF$label = factor(rawDF$label)
```

Next we will create a corpus for our collection of articles using the corpus() function.

```{r}
rawCorpus <- Corpus(VectorSource(rawDF$text))
inspect(rawCorpus[1])
```
The index of each document within the corpus corresponds to the row of the article in our rawDF dataframe.

Next, we will transform all of the text to lowercase and remove the numbers. 

```{r}
cleanCorpus = rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)
```

Next we will use the tm_map function again to remove punctuation and generic words from the document.

```{r}
cleanCorpus = cleanCorpus %>% tm_map(removePunctuation) %>% tm_map(removeWords, stopwords(kind = "en"))
```

We will also remove some additional characters that tm_map(removePunctuation) has failed to remove in previous attempts.

```{r}
toSpace = content_transformer(function (x , pattern ) gsub(pattern, " ", x))

cleanCorpus = tm_map(cleanCorpus, toSpace, "—")
cleanCorpus = tm_map(cleanCorpus, toSpace, " ’s")
cleanCorpus = tm_map(cleanCorpus, toSpace, '“')
cleanCorpus = tm_map(cleanCorpus, toSpace, '”')

inspect(cleanCorpus[1])
```

As we can see from inspecting the first article of the corpus, the text contains a lot of white space where the removed elements used to be. We will also remove this additional white space.

```{r}
cleanCorpus = cleanCorpus %>% tm_map(stripWhitespace)
```
We will compare the clean and raw corpuses to see the results in the following code chunk.

```{r}
tibble(Raw = rawCorpus$content[1:2], Clean = cleanCorpus$content[1:2])
```

Now we will transform the corpus to a matrix. The rows in the matrix are the articles and the columns are the individual words. The numbers in the cells show how many times a given word appears in a given article.

```{r}
cleanDTM = DocumentTermMatrix(cleanCorpus)
inspect(cleanDTM)
```
We will use the createDataPartition() function from the caret package to partition the dataframes into a training set and a testing set. We will be using 80% of the labels for the training set and 20% for the testing set. First we will create a vector of indices for the training set.

```{r}
trainIndex = createDataPartition(rawDF$label, list=FALSE, times = 1, p =.8)
head(trainIndex)
```
Now we will create a new training set from the indices.

```{r}
trainingSet = rawDF[trainIndex, ]
```

Next we will create the test set by subtracting the indices of the training set.

```{r}
testSet = rawDF[-trainIndex, ]
```

We will split the corpus and DTM as well.

```{r}
#trainCorpus = cleanCorpus[trainIndex]
#testCorpus = cleanCorpus[-trainIndex]

trainDTM = cleanDTM[trainIndex, ]
#testDTM = cleanDTM[-trainIndex, ]
```

We know from inspecting the DTM above that it has 217652 terms, which means that it has the same number of features. Using all of these would be computationally inefficient so we will remove the terms with low frequencies as they have little predictive power.

```{r}
freqWords = trainDTM %>% findFreqTerms(500)
trainDTM = DocumentTermMatrix(trainCorpus, list(dictionary = freqWords))
testDTM = DocumentTermMatrix(testCorpus, list(dictionary = freqWords))

head(trainDTM)
```
We can see that we've reduced the number of terms (i.e. features) to 2475.

Now we will transform the counts of each term into a factor that indicates whether the term is present in the article.

```{r}
convert_counts <- function(x) {
  x = ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("0", "1"))
}

trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])
```


## Modeling
text and code here

## Evaluation and Deployment
text and code here

reviewer adds suggestions for improving the model